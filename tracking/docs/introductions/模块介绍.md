[TOC]

# tracking模块

## 类：

### Tracker：





### TrackerBoosting：

#### 算法介绍：

本算法采用基于online Boosting在线学习的算法做目标跟踪。

获取第一个目标的Bounding Box之后，生成一个搜索区域，根据搜索区域和目标的Bounding Box可以训练一个分类器，输入新的帧之后在搜索区域内根据上一次的分类器找到目标，再根据当前跟踪到的目标生成搜索区域，重新训练分类器，重复上述过程。

![Boosting_roi](/home/jerry/Documents/works/tracking/docs/pictures/Boosting_roi.png)

顺序为：a->b->c->d->a

###### 选择ROI区域：

> we have defined by enlarging the target region by one third in each direction (for this region the integral representations are computed).

论文中采用的方法是将Bounding Box四周延长$1/3$，同时计算积分图像和积分直方图。提取ROI区域后再用与上一帧图像相同大小的Bounding Box在ROI区域中滑动，计算不同区域的置信度，置信度最高的区域即为目标。

###### 提取特征：

提取特征的目的是用来生成弱分类器，采用了三种方法提取特征：Haar特征、梯度方向直方图、二值图。

> Note, that the computation of all feature types can be done very efficiently using integral images and integral histograms as data structures.

采用积分图像和积分直方图来提高特征计算的速度。积分图像和积分直方图在提取ROI区域值就开始计算，以减少重复计算。

之后，通过卡尔曼滤波器（Kalman filtering）估计正负样本分布的均值和方差。而求得正负样本的均值和方差后，可以进一步估计正负样本的高斯分布。

###### 计算置信度：

采用online Boosting在线学习的方法。相对于offline Boosting，online Boosting不需要有一个事先训练好的模型，可以在算法的执行过程中训练，这样能提高模型适应性。即，输入一帧图像则训练一次，同时根据训练模型可以给出目标区域。

过程如下：

**弱分类器**：对一个二分类问题，该分类器$h^{weak}$的错误率少于$50\%$，即$e < 0.5$，则可称为弱分类器，它错误率不低，但是执行速度快，且优于随机分类（错误率$50\%$）。强分类器是可以根据实际应用自主选择或设计的。

**选择器**：对$M$个弱分类器的集合$H^{weak} = \{h_1^{weak}, ..., h_M^{weak}\}$，选择器可以选出一个满足下列关系的弱分类器：
$$
h^{select} = h_m^{weak}(X)	\tag1
$$

$$
m = \arg\min_i e_i	\tag2
$$

$e$表示估计误差，上式即选择估计误差最小的一个弱分类器。使用$N$个不同的选择器即可以选择出$N$个不同的弱分类器。对每个选择出来的弱分类器，应该都至少可以对一种特征有比较好的分类效果。

**强分类器：**一个强分类器是$N$个选择器选择出来的弱分类器的线性加权和，通过若干个好的弱分类器的组合，可以认为强分类器可以基本对所有特征有很好的分类效果。
$$
h_{strong} = sign(conf(X))	\tag 3
$$

$$
conf(X) = \sum_{n=1}^N[\alpha _n \cdot h_n^{select}(X)] 	\tag 4
$$

最终输出即为候选区域判定结果。



#### API：

#### demo：





### TrackerCSRT：

#### 算法介绍：

#### API：

#### demo：





### TrackerGOTURN：

#### 算法介绍：

#### API：

#### demo：





### TrackerKCF：

#### 算法介绍：

#### API：

#### demo：





### TrackerMedianFlow：

#### 算法介绍：

#### API：

#### demo：





### TrackerMIL：

MIL全称是Multiple Instance Learning。

#### 算法介绍：

本算法的重点在于正负样本的选取。

在获取第一帧图像的目标的位置（Bounding Box）之后，选取正负样本。该算法的主要思想是把样本打包，只要一个包中包含一个正样本就将该包设定为正样本包。一在后续的训练中，将包中元素作为输入。

###### 正样本：

在目标位置（一个点）一定范围内随机或按一定的函数式（规律）选取若干个样本。满足：
$$
X^s = \{x | < s \ pixels \ from \ tracker \ location\}	\tag 1
$$
论文中$s$取值为$s = 35$，即在目标位置35个像素的圆内取样本，得到的是目标可能的位置。对$X^s$中的所有样本计算新的跟踪位置$p^`$，即根据预设或训练好的模型，选取响应最大的样本：
$$
location_{new} = location(\mathop{\arg\max}_{x \in X^s} p(y = 1|x))	\tag2
$$
在新的跟踪位置$p^`$运用$(1)$设置一个范围$r$，在以$p^`$为圆心，以$r$为半径的圆内选取若干个样本。将样本打包为正样本包，实际上是扩展正样本，但是将正样本包里面的样本判定为正样本是具有不确定性的。

###### 负样本：

在上述以$(2)$确定的跟踪位置为基础上，以$p^`$为圆心，以$\beta$为和$r$为半径（$\beta > r$）做圆环，在圆环内取若干个样本，都记为负样本。每个负样本都是一个包，并且是负样本包。

###### 训练：

仿照Online-Boosting，构造Online-MIL Boost算法。采用Boost的思想，设置$M$个弱分类器。

弱分类器定义如下：
$$
h_k(x) = \log[\frac{p(y = 1 | f_k(x))}{p(y = 0| f_k(x))}]	\tag 3
$$
$f_k$表示特征，并假设样本特征的条件分布满足高斯分布，均值和方差通过新得到的样本更新：
$$
p(f_k(x)|y = 1) ～ N(u_1, \delta_1) \\
p(f_k(x)|y = 0) ～ N(u_0, \delta_0) \\
p(y = 1) = p(y = 0)
$$
对每一次更新迭代，需要选取最合适的（响应最大）几个弱分类器，选择方法如下：
$$
h_k = \arg\max_{h \in \{h_1 ... h_M\}} \log L(H_{k-1} + h)	\tag4
$$
上式中的$H_{k-1}$表示前$k-1$个弱分类器的组合，也就是前$k-1$个好的弱分类器的组合。MIL分类器为所有好的弱分类器的和（下式的$H$（全部）与$(4)$中的$H$（部分）是不同的）：
$$
H(x) = \sum_{k = 1}^Kh_k(x)	\tag5
$$
结果预测采用下式：
$$
p(y = 1|x) = \sigma(H(x))	\tag 6
$$
其中，$\sigma$为sigmod函数：
$$
\sigma(x) = \frac{1}{1+ e^{-x}}
$$

###### 说明：

1. 弱分类器数量设置时，需要满足所有弱分类器数量$M$远大于好的（候选）弱分类器数量$K$；（$M >> K$，论文中取$M = 250, K = 50$）
2. Boost思想之一：每个弱分类器至少对一个特征有很好的区分能力，所以几个弱分类器的组合可以对所有特征有很好的区分能力；
3. 论文中采用的特征为Haar特征并加入颜色信息。



#### 算法流程：

1. 按式（1）选取一系列图像；
2. 通过式（1）选取的图像，按式（2）计算新的追踪位置；
3. 根据新的追踪位置及式（1）选取正负样本；
4. 通过online-MIL Boost训练和更新分类器；
5. 输入新的一帧，通过训练好的分类器及步骤1、2预测确定目标位置。



#### API：



#### demo：





### TrackerMOSSE：

#### 算法介绍：

该算法基于相关性滤波原理：两个信号越相似，相关性越高。目的是通过一个滤波器使目标响应在搜索区域全局最大。

###### 相关性滤波及滤波函数：

对一幅输入图像$f$，有滤波函数$h$，卷积输出$g$满足：
$$
g_i = convolution(f_i, h_i)	\tag 1
$$
时域卷积等于频域相乘，经过傅里叶变换（采用FFT）后（1）可化为：
$$
G_i = F_i \cdot H_i^T \tag2
$$
$G​$、$F​$、$H​$分别表示$g​$、$f​$、$h​$的傅里叶变换。在实际处理中，傅里叶变换之前，采用了以下三步骤：

1. 对每个像素使用$\log$函数计算，以解决低对比度照明的情况；
2. 像素值归一化；
3. 使用cos函数将像素值置为0附近。

按照论文中的说法，上述操作可以加强目标附近的响应强度。傅里叶变换之后得到我们目标滤波器的表达式：
$$
H_i^T = \frac{G_i}{F_i}	\tag3
$$
工程上通过最小误差平方和（Minimum Output Sum of Squared Error，MOSSE）求取$F$：
$$
\min_{H^T} \sum_i|F_i \cdot H^T - G_i|^2	\tag 4
$$
下标$i$表示的是第$i$个样本。使目标函数导数为0，则求得$H^T$表达式为：
$$
H^T = \frac{\sum_i G_i \cdot F_i^T}{\sum_i F_i \cdot F_i^T}	\tag 5
$$
为了提高滤波器的鲁棒性（适应性），对上式做如下改变，使得历史情况也会影响当前跟踪目标的选择：
$$
\begin{eqnarray}
&H^T = \frac{1}{N} \cdot \sum_i \frac{A_i}{B_i}	\tag 6\\
&A_i = \eta \cdot G_i \cdot F_i^T + (1 - \eta) \cdot A_{i-1}	\tag 7\\
&B_i = \eta \cdot F_i \cdot F_i^T + (1 - \eta) \cdot B_{i-1}	\tag 8
\end{eqnarray}
$$
其中，$N$为样本数，$\eta$是学习率，它会给当前帧更多的权重，并随着时间的推移，权重越低。在论文的实验中发现，$\eta = 0.125$是最好的选择。

###### 获取滤波器输入及输出：

> The training set is constructed using random affine transformations to generate eight small perturbations of the tracking window in the initial frame. Training outputs are also generated with their peaks corresponding to the target center.

滤波器输入$f_i$由目标经过随机的仿射变换得到，论文中每次生成$N=8$个输入样本；

输出样本$g_i$通过高斯函数产生，并且其峰值位置是$f_i$的中心。

#### API：

#### demo：





### TrackerTLD：

#### 算法介绍：

#### API：

#### demo：







## 参考资料：

#### TrackerBoosting：

1. [Helmut Grabner, Michael Grabner, and Horst Bischof. Real-time tracking via on-line boosting. In *BMVC*, volume 1, page 6, 2006.](http://www.macs.hw.ac.uk/bmvc2006/papers/033.pdf)
2. [【图像处理】利用积分图像法快速计算Haar特征](https://blog.csdn.net/xiaowei_cqu/article/details/8219324)
3. [目标跟踪学习系列一:on-line boosting and vision 阅读](https://blog.csdn.net/ikerpeng/article/details/18985573)



#### TrackerCSRT：



#### TrackerGOTURN：

1. [David Held, Sebastian Thrun, and Silvio Savarese. Learning to track at 100 fps with deep regression networks. In *European Conference Computer Vision (ECCV)*, 2016.](https://arxiv.org/pdf/1604.01802v1.pdf)



#### TrackerKCF：

1. [J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Exploiting the circulant structure of tracking-by-detection with kernels. In *proceedings of the European Conference on Computer Vision*, 2012.](http://home.isr.uc.pt/~henriques/publications/henriques_eccv2012.pdf)
2. [M. Danelljan, F.S. Khan, M. Felsberg, and J. van de Weijer. Adaptive color attributes for real-time visual tracking. In *Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on*, pages 1090–1097, June 2014.](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Danelljan_Adaptive_Color_Attributes_2014_CVPR_paper.pdf)
3. [High-Speed Tracking with Kernelized Correlation Filters](http://www.robots.ox.ac.uk/~joao/publications/henriques_tpami2015.pdf)
4. [http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html](http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html)



#### TrackerMedianFlow：

1. [Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. Forward-backward error: Automatic detection of tracking failures. In *Pattern Recognition (ICPR), 2010 20th International Conference on*, pages 2756–2759. IEEE, 2010.](https://www.computer.org/csdl/proceedings/icpr/2010/4109/00/4109c756-abs.html)



#### TrackerMIL：

1. [Boris Babenko, Ming-Hsuan Yang, and Serge Belongie. Visual tracking with online multiple instance learning. In *Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on*, pages 983–990. IEEE, 2009.](http://vision.stanford.edu/teaching/cs231b_spring1415/slides/MIL_kelsie.pdf)
2. [目标跟踪学习系列四：on-line multiple instance learning （MIL）学习](https://blog.csdn.net/ikerpeng/article/details/19235391)



#### TrackerMOSSE：

1. [David S. Bolme, J. Ross Beveridge, Bruce A. Draper, and Man Lui Yui. Visual object tracking using adaptive correlation filters. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2010.](http://www.cs.colostate.edu/~draper/papers/bolme_cvpr10.pdf)
2. [相关滤波跟踪（MOSSE）](https://blog.csdn.net/autocyz/article/details/48136473)



#### TrackerTLD：

1. [Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. Tracking-learning-detection. *Pattern Analysis and Machine Intelligence, IEEE Transactions on*, 34(7):1409–1422, 2012.](http://kahlan.eps.surrey.ac.uk/featurespace/tld/Publications/2011_tpami)

